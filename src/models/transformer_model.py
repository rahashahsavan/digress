import math

import torch
import torch.nn as nn
from torch.nn.modules.dropout import Dropout
from torch.nn.modules.linear import Linear
from torch.nn.modules.normalization import LayerNorm
from torch.nn import functional as F
from torch import Tensor

from src import utils
from src.diffusion import diffusion_utils
from src.models.layers import Xtoy, Etoy, masked_softmax


class XEyTransformerLayer(nn.Module):
    """
    Transformer layer that updates node (X), edge (E), and global (y) features.
    
    Args:
        dx: Node feature dimension
        de: Edge feature dimension
        dy: Global feature dimension
        n_head: Number of attention heads
        dim_ffX: Feedforward dimension for node features
        dim_ffE: Feedforward dimension for edge features
        dim_ffy: Feedforward dimension for global features
        dropout: Dropout probability (0 to disable)
        layer_norm_eps: Epsilon for layer normalization
    """
    def __init__(self, dx: int, de: int, dy: int, n_head: int, dim_ffX: int = 2048,
                 dim_ffE: int = 128, dim_ffy: int = 2048, dropout: float = 0.1,
                 layer_norm_eps: float = 1e-5, device=None, dtype=None) -> None:
        kw = {'device': device, 'dtype': dtype}
        super().__init__()

        # Self-attention block for nodes, edges, and global features
        self.self_attn = NodeEdgeBlock(dx, de, dy, n_head, **kw)

        # Node feature processing
        self.linX1 = Linear(dx, dim_ffX, **kw)
        self.linX2 = Linear(dim_ffX, dx, **kw)
        self.normX1 = LayerNorm(dx, eps=layer_norm_eps, **kw)
        self.normX2 = LayerNorm(dx, eps=layer_norm_eps, **kw)
        self.dropoutX1 = Dropout(dropout)
        self.dropoutX2 = Dropout(dropout)
        self.dropoutX3 = Dropout(dropout)

        # Edge feature processing
        self.linE1 = Linear(de, dim_ffE, **kw)
        self.linE2 = Linear(dim_ffE, de, **kw)
        self.normE1 = LayerNorm(de, eps=layer_norm_eps, **kw)
        self.normE2 = LayerNorm(de, eps=layer_norm_eps, **kw)
        self.dropoutE1 = Dropout(dropout)
        self.dropoutE2 = Dropout(dropout)
        self.dropoutE3 = Dropout(dropout)

        # Global feature processing
        self.lin_y1 = Linear(dy, dim_ffy, **kw)
        self.lin_y2 = Linear(dim_ffy, dy, **kw)
        self.norm_y1 = LayerNorm(dy, eps=layer_norm_eps, **kw)
        self.norm_y2 = LayerNorm(dy, eps=layer_norm_eps, **kw)
        self.dropout_y1 = Dropout(dropout)
        self.dropout_y2 = Dropout(dropout)
        self.dropout_y3 = Dropout(dropout)

        self.activation = F.relu

    def forward(self, X: Tensor, E: Tensor, y, node_mask: Tensor):
        """
        Forward pass through the transformer layer.
        
        Args:
            X: Node features (bs, n, dx)
            E: Edge features (bs, n, n, de)
            y: Global features (bs, dy)
            node_mask: Mask for valid nodes (bs, n)
            
        Returns:
            Updated X, E, y with same shapes
        """
        # Self-attention
        newX, newE, new_y = self.self_attn(X, E, y, node_mask=node_mask)

        # Residual connection + layer norm for X
        newX_d = self.dropoutX1(newX)
        X = self.normX1(X + newX_d)

        # Residual connection + layer norm for E
        newE_d = self.dropoutE1(newE)
        E = self.normE1(E + newE_d)

        # Residual connection + layer norm for y
        new_y_d = self.dropout_y1(new_y)
        y = self.norm_y1(y + new_y_d)

        # Feedforward network for X
        ff_outputX = self.linX2(self.dropoutX2(self.activation(self.linX1(X))))
        ff_outputX = self.dropoutX3(ff_outputX)
        X = self.normX2(X + ff_outputX)

        # Feedforward network for E
        ff_outputE = self.linE2(self.dropoutE2(self.activation(self.linE1(E))))
        ff_outputE = self.dropoutE3(ff_outputE)
        E = self.normE2(E + ff_outputE)

        # Feedforward network for y
        ff_output_y = self.lin_y2(self.dropout_y2(self.activation(self.lin_y1(y))))
        ff_output_y = self.dropout_y3(ff_output_y)
        y = self.norm_y2(y + ff_output_y)

        return X, E, y


class NodeEdgeBlock(nn.Module):
    """
    Self-attention layer that updates node and edge representations jointly.
    Uses FiLM (Feature-wise Linear Modulation) to incorporate edge and global features.
    """
    def __init__(self, dx, de, dy, n_head, **kwargs):
        super().__init__()
        assert dx % n_head == 0, f"dx: {dx} -- nhead: {n_head}"
        self.dx = dx
        self.de = de
        self.dy = dy
        self.df = int(dx / n_head)  # Feature dimension per head
        self.n_head = n_head

        # Attention projections
        self.q = Linear(dx, dx)
        self.k = Linear(dx, dx)
        self.v = Linear(dx, dx)

        # FiLM layers: Edge -> Node attention modulation
        self.e_add = Linear(de, dx)
        self.e_mul = Linear(de, dx)

        # FiLM layers: Global -> Edge modulation
        self.y_e_mul = Linear(dy, dx)  # Note: dx not de for compatibility
        self.y_e_add = Linear(dy, dx)

        # FiLM layers: Global -> Node modulation
        self.y_x_mul = Linear(dy, dx)
        self.y_x_add = Linear(dy, dx)

        # Global feature processing
        self.y_y = Linear(dy, dy)
        self.x_y = Xtoy(dx, dy)  # Aggregate node features to global
        self.e_y = Etoy(de, dy)  # Aggregate edge features to global

        # Output projections
        self.x_out = Linear(dx, dx)
        self.e_out = Linear(dx, de)
        self.y_out = nn.Sequential(nn.Linear(dy, dy), nn.ReLU(), nn.Linear(dy, dy))

    def forward(self, X, E, y, node_mask):
        """
        Forward pass with joint node-edge attention.
        
        Args:
            X: Node features (bs, n, dx)
            E: Edge features (bs, n, n, de)
            y: Global features (bs, dy)
            node_mask: Valid node mask (bs, n)
            
        Returns:
            Updated X, E, y
        """
        bs, n, _ = X.shape
        
        # Create masks for nodes and edges
        x_mask = node_mask.unsqueeze(-1)        # (bs, n, 1)
        e_mask1 = x_mask.unsqueeze(2)           # (bs, n, 1, 1)
        e_mask2 = x_mask.unsqueeze(1)           # (bs, 1, n, 1)

        # 1. Compute queries and keys
        Q = self.q(X) * x_mask                  # (bs, n, dx)
        K = self.k(X) * x_mask                  # (bs, n, dx)
        diffusion_utils.assert_correctly_masked(Q, x_mask)
        
        # 2. Reshape to multi-head format
        Q = Q.reshape((Q.size(0), Q.size(1), self.n_head, self.df))
        K = K.reshape((K.size(0), K.size(1), self.n_head, self.df))

        Q = Q.unsqueeze(2)                      # (bs, 1, n, n_head, df)
        K = K.unsqueeze(1)                      # (bs, n, 1, n_head, df)

        # Compute unnormalized attention scores
        Y = Q * K
        Y = Y / math.sqrt(Y.size(-1))
        diffusion_utils.assert_correctly_masked(Y, (e_mask1 * e_mask2).unsqueeze(-1))

        # 3. Modulate attention with edge features (FiLM)
        E1 = self.e_mul(E) * e_mask1 * e_mask2                        # (bs, n, n, dx)
        E1 = E1.reshape((E.size(0), E.size(1), E.size(2), self.n_head, self.df))

        E2 = self.e_add(E) * e_mask1 * e_mask2                        # (bs, n, n, dx)
        E2 = E2.reshape((E.size(0), E.size(1), E.size(2), self.n_head, self.df))

        # Incorporate edge features into attention scores
        Y = Y * (E1 + 1) + E2                   # (bs, n, n, n_head, df)

        # 4. Compute new edge features
        newE = Y.flatten(start_dim=3)           # (bs, n, n, dx)
        
        # Modulate edges with global features (FiLM)
        ye1 = self.y_e_add(y).unsqueeze(1).unsqueeze(1)  # (bs, 1, 1, dx)
        ye2 = self.y_e_mul(y).unsqueeze(1).unsqueeze(1)
        newE = ye1 + (ye2 + 1) * newE

        # Output projection for edges
        newE = self.e_out(newE) * e_mask1 * e_mask2      # (bs, n, n, de)
        diffusion_utils.assert_correctly_masked(newE, e_mask1 * e_mask2)

        # 5. Compute attention weights and apply to values
        softmax_mask = e_mask2.expand(-1, n, -1, self.n_head)
        attn = masked_softmax(Y, softmax_mask, dim=2)    # (bs, n, n, n_head)

        V = self.v(X) * x_mask                           # (bs, n, dx)
        V = V.reshape((V.size(0), V.size(1), self.n_head, self.df))
        V = V.unsqueeze(1)                               # (bs, 1, n, n_head, df)

        # Compute attention-weighted values
        weighted_V = attn * V
        weighted_V = weighted_V.sum(dim=2)
        weighted_V = weighted_V.flatten(start_dim=2)     # (bs, n, dx)

        # 6. Modulate node features with global features (FiLM)
        yx1 = self.y_x_add(y).unsqueeze(1)
        yx2 = self.y_x_mul(y).unsqueeze(1)
        newX = yx1 + (yx2 + 1) * weighted_V

        # Output projection for nodes
        newX = self.x_out(newX) * x_mask
        diffusion_utils.assert_correctly_masked(newX, x_mask)

        # 7. Update global features by aggregating node and edge information
        y = self.y_y(y)
        e_y = self.e_y(E)
        x_y = self.x_y(X)
        new_y = y + x_y + e_y
        new_y = self.y_out(new_y)               # (bs, dy)

        return newX, newE, new_y


class GraphTransformer(nn.Module):
    """
    Graph Transformer model with multiple XEyTransformerLayer blocks.
    Processes node features (X), edge features (E), and global features (y).
    
    Args:
        n_layers: Number of transformer layers
        input_dims: Dict with 'X', 'E', 'y' input dimensions
        hidden_mlp_dims: Dict with 'X', 'E', 'y' hidden dimensions for MLPs
        hidden_dims: Dict with transformer hidden dimensions
        output_dims: Dict with 'X', 'E', 'y' output dimensions
        act_fn_in: Activation function for input MLPs
        act_fn_out: Activation function for output MLPs
    """
    def __init__(self, n_layers: int, input_dims: dict, hidden_mlp_dims: dict, hidden_dims: dict,
                 output_dims: dict, act_fn_in: nn.ReLU(), act_fn_out: nn.ReLU()):
        super().__init__()
        self.n_layers = n_layers
        self.out_dim_X = output_dims['X']
        self.out_dim_E = output_dims['E']
        self.out_dim_y = output_dims['y']

        # Input projection MLPs
        self.mlp_in_X = nn.Sequential(
            nn.Linear(input_dims['X'], hidden_mlp_dims['X']), 
            act_fn_in,
            nn.Linear(hidden_mlp_dims['X'], hidden_dims['dx']), 
            act_fn_in
        )

        self.mlp_in_E = nn.Sequential(
            nn.Linear(input_dims['E'], hidden_mlp_dims['E']), 
            act_fn_in,
            nn.Linear(hidden_mlp_dims['E'], hidden_dims['de']), 
            act_fn_in
        )

        self.mlp_in_y = nn.Sequential(
            nn.Linear(input_dims['y'], hidden_mlp_dims['y']), 
            act_fn_in,
            nn.Linear(hidden_mlp_dims['y'], hidden_dims['dy']), 
            act_fn_in
        )

        # Stack of transformer layers
        self.tf_layers = nn.ModuleList([
            XEyTransformerLayer(
                dx=hidden_dims['dx'],
                de=hidden_dims['de'],
                dy=hidden_dims['dy'],
                n_head=hidden_dims['n_head'],
                dim_ffX=hidden_dims['dim_ffX'],
                dim_ffE=hidden_dims['dim_ffE']
            )
            for i in range(n_layers)
        ])

        # Output projection MLPs
        self.mlp_out_X = nn.Sequential(
            nn.Linear(hidden_dims['dx'], hidden_mlp_dims['X']), 
            act_fn_out,
            nn.Linear(hidden_mlp_dims['X'], output_dims['X'])
        )

        self.mlp_out_E = nn.Sequential(
            nn.Linear(hidden_dims['de'], hidden_mlp_dims['E']), 
            act_fn_out,
            nn.Linear(hidden_mlp_dims['E'], output_dims['E'])
        )

        self.mlp_out_y = nn.Sequential(
            nn.Linear(hidden_dims['dy'], hidden_mlp_dims['y']), 
            act_fn_out,
            nn.Linear(hidden_mlp_dims['y'], output_dims['y'])
        )

    def forward(self, X, E, y, node_mask):
        """
        Forward pass through the graph transformer.
        
        Args:
            X: Node features (bs, n, dx)
            E: Edge features (bs, n, n, de)
            y: Global features (bs, dy)
            node_mask: Valid node mask (bs, n)
            
        Returns:
            PlaceHolder object with updated X, E, y
        """
        bs, n = X.shape[0], X.shape[1]

        # Create diagonal mask (no self-loops)
        diag_mask = torch.eye(n)
        diag_mask = ~diag_mask.type_as(E).bool()
        diag_mask = diag_mask.unsqueeze(0).unsqueeze(-1).expand(bs, -1, -1, -1)

        # Store input for residual connections
        X_to_out = X[..., :self.out_dim_X]
        E_to_out = E[..., :self.out_dim_E]
        y_to_out = y[..., :self.out_dim_y]

        # Input projection
        new_E = self.mlp_in_E(E)
        new_E = (new_E + new_E.transpose(1, 2)) / 2  # Ensure edge symmetry
        
        after_in = utils.PlaceHolder(
            X=self.mlp_in_X(X), 
            E=new_E, 
            y=self.mlp_in_y(y)
        ).mask(node_mask)
        
        X, E, y = after_in.X, after_in.E, after_in.y

        # Apply transformer layers
        for layer in self.tf_layers:
            X, E, y = layer(X, E, y, node_mask)

        # Output projection
        X = self.mlp_out_X(X)
        E = self.mlp_out_E(E)
        y = self.mlp_out_y(y)

        # Residual connections
        X = (X + X_to_out)
        E = (E + E_to_out) * diag_mask  # Apply diagonal mask
        y = y + y_to_out

        # Ensure edge symmetry
        E = 1/2 * (E + torch.transpose(E, 1, 2))

        return utils.PlaceHolder(X=X, E=E, y=y).mask(node_mask)

    def freeze_transformer_layers(self):
        """
        Freeze all transformer layers for transfer learning.
        Only input/output MLPs remain trainable.
        This is useful when adding new features (e.g., Ricci curvature) 
        and you want to preserve learned representations.
        """
        # Freeze all transformer layers
        for layer in self.tf_layers:
            for param in layer.parameters():
                param.requires_grad = False
        
        # Ensure input/output MLPs are trainable
        for param in self.mlp_in_X.parameters():
            param.requires_grad = True
        for param in self.mlp_in_E.parameters():
            param.requires_grad = True
        for param in self.mlp_in_y.parameters():
            param.requires_grad = True
        for param in self.mlp_out_X.parameters():
            param.requires_grad = True
        for param in self.mlp_out_E.parameters():
            param.requires_grad = True
        for param in self.mlp_out_y.parameters():
            param.requires_grad = True
        
        # Count frozen vs trainable parameters
        frozen_params = sum(p.numel() for p in self.parameters() if not p.requires_grad)
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        print("\n" + "="*80)
        print("ðŸ”’ TRANSFORMER LAYERS FROZEN FOR TRANSFER LEARNING")
        print("="*80)
        print(f"Frozen parameters:     {frozen_params:>12,d}")
        print(f"Trainable parameters:  {trainable_params:>12,d}")
        print(f"Total parameters:      {frozen_params + trainable_params:>12,d}")
        print(f"Trainable ratio:       {100 * trainable_params / (frozen_params + trainable_params):>11.1f}%")
        print("="*80 + "\n")